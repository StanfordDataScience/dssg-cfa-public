{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Bulk converting Thea's jsons into T's segments.\n",
    "# \n",
    "# Written primarily by Robbie thus far.\n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for others to use this script, it will help to change this variable to\n",
    "# whatever the route it to the root of your dssg-cfa folder.\n",
    "ROUTETOROOTDIR = '/home/dssg-cfa/notebooks/dssg-cfa-public/'\n",
    "IMPORTSCRIPTSDIR = ROUTETOROOTDIR + \"util/py_files\"\n",
    "UTILDIR = ROUTETOROOTDIR + 'util'\n",
    "JSONSDIR = ROUTETOROOTDIR + 'A_pdf_to_text/jsons_ke_gazettes/'\n",
    "CSVTRAINDIR = ROUTETOROOTDIR + 'B_text_preprocessing/csv_outputs_train/'\n",
    "CSVTESTDIR = ROUTETOROOTDIR + 'B_text_preprocessing/csv_outputs_test/'\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.chdir(IMPORTSCRIPTSDIR)\n",
    "import retoolingSegmentation\n",
    "import orderingText\n",
    "import setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfJsons(filepath = JSONSDIR):\n",
    "    \"\"\"Get a list of all the filenames of all gazettes in our database.\n",
    "    \n",
    "    args:\n",
    "    filepath: filepath to the directory to search for gazette jsons in.\n",
    "    \n",
    "    returns: a list of all the filenames of all gazettes in our database\"\"\"\n",
    "    \n",
    "    os.chdir(filepath)\n",
    "    ret = !ls\n",
    "    return ret\n",
    "\n",
    "# load filenames into a list for future use\n",
    "listOfJsons = getListOfJsons()\n",
    "os.chdir(IMPORTSCRIPTSDIR)\n",
    "\n",
    "def readJsonIntoDict(jsonNum, pageNum = 'all', filepath = JSONSDIR):\n",
    "    \"\"\"Read a json from Read API into a Python dictionary and return it.\n",
    "    \n",
    "    args:\n",
    "    jsonNum: The index of the gazette json by alphabetical order to convert and return.\n",
    "    pageNum: The page number of the gazette to return in dictionary format.\n",
    "        If pageNum == 'all', then return all pages.\n",
    "        \n",
    "    returns: A highly nested Python dictionary from Read API's json output.\n",
    "    To understand the structure of this dictionary better, see Microsoft's help pages.\"\"\"\n",
    "    \n",
    "    os.chdir(filepath)\n",
    "    filename = listOfJsons[jsonNum]\n",
    "    with open(filename) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    pages_list = data['analyzeResult']['readResults']\n",
    "    if pageNum == 'all':\n",
    "        return pages_list\n",
    "    else:\n",
    "        return pages_list[pageNum]['lines']\n",
    "    \n",
    "def getLines(jsonDict, pageNum):\n",
    "    \"\"\"Given a dictionary from Read API which contains all pages and a page number, \n",
    "    return the lines from that page num in Python dictionary format.\n",
    "    \n",
    "    args:\n",
    "    jsonDict: a Python dictionary from a call to Read API of a complete gazette.\n",
    "    pageNum: the page whose lines will be returned.\n",
    "    \n",
    "    returns: A highly nested Python dictionary from Read API's json output.\n",
    "    To understand the structure of this dictionary better, see Microsoft's help pages.\"\"\"\n",
    "    \n",
    "    return jsonDict[pageNum]['lines']\n",
    "\n",
    "def getNumPages(jsonDict):\n",
    "    \"\"\"Given a dictionary from Read API, return the number of pages it has.\n",
    "    \n",
    "    args:\n",
    "    jsonDict: a Python dictionary from a call to Read API of a complete gazette.\n",
    "    \n",
    "    returns: the number of pages in the (json of the) gazette.\"\"\"\n",
    "    \n",
    "    numPages = 0\n",
    "    working = True\n",
    "    while working:\n",
    "        try:\n",
    "            useless = jsonDict[numPages]['lines']\n",
    "            numPages += 1\n",
    "        except:\n",
    "            working = False\n",
    "    return numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPage(jsonDict, pageNum, keepPageHeader = False, includeTables = False, cleaningFns = []):\n",
    "    \"\"\"Given a json dict of a gazette, read the text of a page into one string and return it.\n",
    "    \n",
    "    args:\n",
    "    jsonDict: dictionary representing an entire gazette\n",
    "    pageNum: page number to read\n",
    "    keepPageHeader: If True keep the three items appearing at the top of each page \n",
    "            (date, \"The Kenya Gazette\", page num)\n",
    "    includeTables: if True, include the transcription of pages which look like tables (>2 columns).\n",
    "         Otherwise, return the empty string for table pages.\n",
    "    cleaningFNs: functions to call on the text to clean it up (ie replacing 'No.' with 'number')\n",
    "    \n",
    "    returns: the cleaned and ordered text of one gazette page.\"\"\"\n",
    "    \n",
    "    page_lines = getLines(jsonDict, pageNum)\n",
    "    if len(page_lines) < 20:\n",
    "        # not enough lines on this page, don't bother with it.\n",
    "        return ''\n",
    "    if pageNum == 0:\n",
    "        text = orderingText.readTitlePage(page_lines)\n",
    "    else:\n",
    "        numCols = orderingText.getNumCols(page_lines)\n",
    "        if numCols == None or numCols > 2:\n",
    "            if includeTables:\n",
    "                text = orderingText.readTablePage(page_lines)\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            text = orderingText.read2ColPagePreserveParagraphs(page_lines)\n",
    "    for fn in cleaningFns:\n",
    "        text = fn(text)\n",
    "    return text\n",
    "\n",
    "def readEntireGazette(jsonNum, keepPageHeader = False, includeTables = False, cleaningFns = []):\n",
    "    \"\"\"Read the text of an entire gazette into one string and return it (in order).\n",
    "    \n",
    "    args:\n",
    "    jsonNum: json index number of the gazette to read (in jsonList).\n",
    "    keepPageHeader: If True keep the three items appearing at the top of each page \n",
    "            (date, \"The Kenya Gazette\", page num).\n",
    "    includeTables: if True, include the transcription of pages which look like tables (>2 columns).\n",
    "         Otherwise, return the empty string for table pages.\n",
    "    cleaningFNs: functions to call on the text to clean it up (ie replacing 'No.' with 'number').\n",
    "    \n",
    "    returns: the cleaned and ordered text of one gazette.\"\"\"\n",
    "    \n",
    "    jsonDict = readJsonIntoDict(jsonNum)\n",
    "    numPages = getNumPages(jsonDict)\n",
    "    ret = ''\n",
    "    for pageNum in range(0, numPages):\n",
    "        ret += readPage(jsonDict, pageNum, keepPageHeader, includeTables, cleaningFns)\n",
    "    return ret\n",
    "\n",
    "def writeEntireGazetteToCsv(jsonNum, filepath = 'default',\n",
    "                       filename = 'default', keepPageHeader = False, includeTables = False, \n",
    "                       cleaningFns = [], includeSpecial = False, includeNonLRA = False,\n",
    "                       startYear = 2017, endYear = 2020):\n",
    "    \"\"\"Write into csv format an entire gazette. Extract named entities using regexes.\n",
    "    Write only certain segments to csv format.\n",
    "    \n",
    "    args:\n",
    "    jsonNum: json index number of the gazette to read (in jsonList).\n",
    "    filepath: filepath to write the csv to.\n",
    "    filename: file name to write the csv to.\n",
    "    keepPageHeader: If True keep the three items appearing at the top of each page \n",
    "            (date, \"The Kenya Gazette\", page num)\n",
    "    includeTables: if True, include the transcription of pages which look like tables (>2 columns).\n",
    "         Otherwise, return the empty string for table pages.\n",
    "    cleaningFNs: functions to call on the text to clean it up (ie replacing 'No.' with 'number').\n",
    "    includeSpecial: If False, do not write a csv for a gazette whose title includese the word 'special'.\n",
    "    includeNonLRA: If True, include all land-related seg\n",
    "    startYear, endYear: If the gazette was published within this range of years (inclusive), \n",
    "        write it to csv. Otherwise, do not.\n",
    "    \n",
    "    returns: a pandas dataframe with regex-extracted entities by segment. \n",
    "        Will return 0 if no segments are found. Includes only segments that are land-related\n",
    "        if includeNonLRA is true, and only segments with the header 'THELAND REGISTRATION ACT' \n",
    "        if includeNonLRA is false. \"\"\"\n",
    "    \n",
    "    if filepath == 'default':\n",
    "        filepath = CSVTRAINDIR\n",
    "    if filename == 'default':\n",
    "        filename = 'entities_' + listOfJsons[jsonNum]\n",
    "        \n",
    "    if 'special' in filename and not includeSpecial:\n",
    "        return\n",
    "    \n",
    "    goodYear = False\n",
    "    for year in range(startYear, endYear + 1):\n",
    "        if str(year) in filename:\n",
    "            goodYear = True\n",
    "            break\n",
    "            \n",
    "    if not goodYear:\n",
    "        return\n",
    "        \n",
    "    text = readEntireGazette(jsonNum, keepPageHeader, includeTables, cleaningFns)\n",
    "    return retoolingSegmentation.writeEntitiesToCsv(text, filename, filepath, includeNonLRA = includeNonLRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeGroupOfGazettesToCsv(startI, endI, filepath = 'default',\n",
    "                         filename = 'default', keepPageHeader = False, includeTables = False, \n",
    "                         cleaningFns = [], includeSpecial = False, includeNonLRA = False,\n",
    "                         startYear = 2017, endYear = 2020):\n",
    "    \"\"\"Write into csv format a range of gazettes. Extract named entities using regexes.\n",
    "    Write only certain segments to csv format, and only from certain gazettes.\n",
    "    \n",
    "    args:\n",
    "    startI, endI: attempt to write to csvs gazettes from this range of indices in jsonList.\n",
    "        Include startI, do not include endI.\n",
    "    filepath: filepath to write the csv to.\n",
    "    filename: file name to write the csv to.\n",
    "    keepPageHeader: If True keep the three items appearing at the top of each page \n",
    "            (date, \"The Kenya Gazette\", page num)\n",
    "    includeTables: if True, include the transcription of pages which look like tables (>2 columns).\n",
    "         Otherwise, return the empty string for table pages.\n",
    "    cleaningFNs: functions to call on the text to clean it up (ie replacing 'No.' with 'number').\n",
    "    includeSpecial: If False, do not write a csv for a gazette whose title includese the word 'special'.\n",
    "    includeNonLRA: If True, include all land-related seg\n",
    "    startYear, endYear: If the gazette was published within this range of years (inclusive), \n",
    "        write it to csv. Otherwise, do not. \"\"\"\n",
    "    \n",
    "    for i in range(startI, endI):\n",
    "        writeEntireGazetteToCsv(i, filepath, filename, startYear = startYear, endYear = endYear, \n",
    "                                includeNonLRA = includeNonLRA, includeSpecial = includeSpecial,\n",
    "                                keepPageHeader = keepPageHeader, includeTables = includeTables,\n",
    "                                cleaningFns = cleaningFNs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGazetteNumByName(searchName):\n",
    "    \"\"\"Find the indices of gazettes whose names contain the string given in searchName\n",
    "    from the global variable listOfJsons.\n",
    "    \n",
    "    args: \n",
    "    searchname: (partial) name to search for in jsonList.\n",
    "    \n",
    "    returns: a list of indices which match the search query.\"\"\"\n",
    "    \n",
    "    ret = []\n",
    "    for i in range(len(listOfJsons)):\n",
    "        jsonName = listOfJsons[i]\n",
    "        if searchName in jsonName:\n",
    "            ret.append(i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTrainSet():\n",
    "    \"\"\"Write to csv all gazettes that we will use to train out spaCy model.\n",
    "    This train set is gazettes between 2017 and 2020, only including land registration\n",
    "    act segments.\"\"\"\n",
    "    \n",
    "    writeGroupOfGazettesToCsv(0, len(listOfJsons), startYear = 2017, endYear = 2020, \n",
    "                              includeNonLRA = False, filepath = CSVTRAINDIR)\n",
    "    \n",
    "def writeAllGazettes():\n",
    "    \"\"\"Write to csv all gazettes from 2012 to 2020. Inlcude all land-related segments.\"\"\"\n",
    "    \n",
    "    writeGroupOfGazettesToCsv(0, len(listOfJsons), startYear = 2012, endYear = 2020, \n",
    "                              includeNonLRA = True, filepath = CSVTESTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
