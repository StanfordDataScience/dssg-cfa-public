{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "from io import BytesIO\n",
    "\n",
    "# for regular expression\n",
    "import re\n",
    "\n",
    "ROUTETOROOTDIR = '/home/dssg-cfa/notebooks/dssg-cfa-public/'\n",
    "IMPORTSCRIPTSDIR_A = ROUTETOROOTDIR + \"A_pdf_to_text/py_files\"\n",
    "os.chdir(IMPORTSCRIPTSDIR_A)\n",
    "import write_urls as wu\n",
    "import dest_fn_from_url as df\n",
    "import json_extraction as je\n",
    "import check_gazette_filenames as cf\n",
    "\n",
    "\n",
    "# final dest urls -- change these to where you'd like to save these files to\n",
    "final_dest_file_gazeti = \"/home/dssg-cfa/final_dest_urls/final_dest_urls_gazeti.txt\"\n",
    "final_dest_file_ca = \"/home/dssg-cfa/final_dest_urls/final_dest_urls_ca.txt\"\n",
    "\n",
    "# file to save to \n",
    "filepath_out = \"/home/dssg-cfa/ke-gazettes/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **GET URLS**\n",
    "\n",
    "Notes: \n",
    "* Write out to files, rather than saving in local variable, to avoid having to do this again if something goes wrong\n",
    "* Note that each of these took us around 20 minutes to run per 1,000 gazettes -- a possible future improvement would be to run these using multiprocessor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For documentation, see \"write_urls\"\n",
    "\n",
    "wu.write_dest_urls(\"gazeti\", final_dest_file_gazeti, yr_start = 2000, yr_end = 2009)\n",
    "wu.write_dest_urls(\"connected_africa\", final_dest_file_ca, yr_start = 2000, yr_end = 2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(final_dest_file_ca) as fp:\n",
    "    final_dest_ca = [line.strip() for line in fp.readlines()]\n",
    "\n",
    "with open(final_dest_file_gazeti) as fp: \n",
    "    final_dest_gazeti = [line.strip() for line in fp.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Send each URL to Microsoft Read API (or other OCR service)** \n",
    "\n",
    "We do this in batches (iterating through sublists) to enable some level of manual error checking. \n",
    "\n",
    "The below will run as a loop, printing progress updates as it runs. (Multiprocessing does not improve performance here, since there is a limit on calls per second in any Microsoft Cognitive Services subscription.) \n",
    "\n",
    "The below **saves failed gazette URLs to a temporary file**, to be processed later. When the Read API fails on a PDF, our script prints a message describing the error, then saves the URL to the list of failed URLs. At the end of the loop, these failed URLs are saved to a file. Please review step 3 below for how we get these PDFs. \n",
    "\n",
    "Duplicate filenames will not be sent to the Read API, so you will not be charged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE\n",
    "failed_outfile = \"failures.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: \n",
    "* Set `img_url_list` to the full list that you want to process. (We separated these between Gazeti and Connected Africa, but you may wish to concatenate the two.) \n",
    "* Use the `start_idx` and `end_idx` variables to process only a small number of elements at a time. \n",
    "    + E.g., to test the functions on one URL, set `start_idx = 0` and `end_idx = 1`. \n",
    "    + After processing this URL, you may wish to test the function on 10 URLs. To do this, change the values of the variables: `start_idx = 1` and `end_idx = 11`. \n",
    "    + Keep going, with bigger numbers as you grow more confident that things are working. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_url_list = final_dest_gazeti\n",
    "img_url_list = final_dest_ca\n",
    "\n",
    "# Toggle these to loop through the list\n",
    "start_idx = 0\n",
    "end_idx = len(img_url_list) - 1\n",
    "fin_url_sublist = img_url_list[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the loop for a single sub-list \n",
    "\n",
    "duplicates = []\n",
    "failures = []\n",
    "\n",
    "je.bulk_ocr(\n",
    "    fin_url_sublist, \n",
    "    duplicates, \n",
    "    failures,\n",
    "    flag = \"url\"\n",
    ")\n",
    "\n",
    "# after each iteration, append URLs to the file listing failed URLs\n",
    "# if you wish to examine duplicates, you may wish to save these as well\n",
    "with open(failed_outfile, 'a') as f: \n",
    "    f.writelines(\"%s\\n\" % item for item in failures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Get JSONs for Gazettes that the Read API failed on in the above code.**\n",
    "\n",
    "Often, the Read API will fail on a Gazette for one of two reasons: \n",
    "* The URL redirects rather than pointing directly to the Gazette. \n",
    "* The PDF contains pages that are larger than 17x17 inches, which is the maximum size that the Read API will process. \n",
    "* The URL itself is invalid -- there is no PDF for a given Gazette stored in the database. \n",
    "\n",
    "The first two of these issues can be solved by downloading the PDF data directly, resizing each page as necessary, and then sending that data as bytes directly to the Read API. We do not do this for all of our PDFs because it is much more computationally expensive than just passing a URL to the Read API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option to save the invalid URLs if you'd like to analyze them\n",
    "\n",
    "invalid_urls = []\n",
    "with open(failed_outfile) as f: \n",
    "    failed_urls = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "je.bulk_ocr(\n",
    "    failures, \n",
    "    duplicates, \n",
    "    invalid_urls, \n",
    "    flag = \"pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "* By this weekend (August 22nd), I will link from here additional walkthroughs, depending on results of code reviews and feedback on what is confusing.\n",
    "* I will also add in scripts for checking errors in the Gazettes, but I am still finishing and debugging these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
