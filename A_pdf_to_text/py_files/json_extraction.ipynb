{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions to call the Microsoft Cognitive Services APIs to get and \n",
    "save OCR'd JSON outputs of one or multiple files. \n",
    "'''\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import PyPDF2\n",
    "import io\n",
    "import re\n",
    "\n",
    "import dest_fn_from_url as df\n",
    "\n",
    "# 17 inches (max for Read API) x 72 points per inch\n",
    "MAX_DIM = 17*72\n",
    "\n",
    "# rigorously commented for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_read_api(final_dest_url = \"\", flag = \"url\", image_path = \"\"):\n",
    "    '''    \n",
    "    Function to call Microsoft cognitive services Read API on a single gazette (all pages).\n",
    "    \n",
    "    Flag should be \"url\" or \"pdf\". \n",
    "    If pdf, provide path to the PDF file. \n",
    "    If URL, provide url which points to the PDF file (does not redirect).\n",
    "\n",
    "    Returns: \n",
    "    1. JSON-formatted output of the analysis\n",
    "    2. Boolean for whether the analysis was successful\n",
    "    (If analysis was unsuccessful, the output of the analysis returned will be JSON-formatted\n",
    "    and include the text of the error message)\n",
    "    '''\n",
    "\n",
    "    # unique to our Computer Vision resource (\"keys and endpoints\" tab)\n",
    "    endpoint = \"<YOUR ENDPOINT HERE>\"\n",
    "    subscription_key = \"<YOUR SUBSCRIPTION KEY HERE>\"\n",
    "\n",
    "    text_recognition_url = endpoint + \"/vision/v3.0/read/analyze\"\n",
    "\n",
    "    # ---------------------\n",
    "    # Extracting text requires two API calls: One call to submit the\n",
    "    # image for processing, the other to retrieve the text found in the image. \n",
    "    # ---------------------\n",
    "    \n",
    "    # ------ FIRST API CALL: SUBMIT THE IMAGE ------ \n",
    "    \n",
    "    # content-type is the type of data sent to the API (octet-stream for bytes; json for url)\n",
    "    \n",
    "    if flag == \"url\":\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Ocp-Apim-Subscription-Key': subscription_key\n",
    "        }\n",
    "\n",
    "        # posts request to the text_recognition_url (sending data to server)\n",
    "        response = requests.post(text_recognition_url, headers=headers, json={'url': final_dest_url})\n",
    "        \n",
    "    elif flag == \"pdf\": \n",
    "        headers = {\n",
    "            'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "            'Content-Type': 'application/octet-stream'\n",
    "        }\n",
    "\n",
    "        pdf_data = open(image_path, 'rb').read()\n",
    "        response = requests.post(text_recognition_url, headers=headers, data=pdf_data)\n",
    "        \n",
    "    else: \n",
    "        return \"Bad flag; should be \\'pdf\\' or \\'url\\'\", False \n",
    "    \n",
    "    # Return false if call did not go through \n",
    "    if response.status_code != 202: \n",
    "        msg = \"Bad request URL/PDF for \" + final_dest_url\n",
    "        return msg, False\n",
    "\n",
    "\n",
    "    # ------ SECOND API CALL: RETRIEVE THE TEXT FROM THE SERVER ------ \n",
    "    \n",
    "    # (Our request returns a response object, which has information)\n",
    "    # The response object contains the URL used to retrieve the recognized text.\n",
    "    # This is the URL where our recognized text is currently \"stored\" on Microsoft's end\n",
    "    operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "    # The recognized text isn't immediately available, so poll to wait for completion.\n",
    "    analysis = {}\n",
    "    poll = True\n",
    "    while (poll):\n",
    "        # (GET request means that we're getting data from a server)\n",
    "        # Same headers as before -- our credentials & we want the output in JSON \n",
    "        response_final = requests.get(\n",
    "            response.headers[\"Operation-Location\"], headers=headers)\n",
    "        # store the JSON format response in analysis \n",
    "        analysis = response_final.json()\n",
    "\n",
    "        # when complete, the analysis object will have an \"analyzeResult\" element\n",
    "        if (\"analyzeResult\" in analysis):\n",
    "            poll = False\n",
    "        \n",
    "        # if the analysis failed \n",
    "        if (\"status\" in analysis and analysis['status'] == 'failed'):\n",
    "            return analysis, False \n",
    "        if (\"error\" in analysis):\n",
    "            return analysis, False\n",
    "        \n",
    "        # limit calls to our subscription data rate (10 calls per second)\n",
    "        time.sleep(0.1)\n",
    "         \n",
    "    return analysis, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_read_api_resize(final_dest_url, temp_pdf_fp, firstPageOnly = False): \n",
    "    '''\n",
    "    This is a much slower way to get OCR'd version of a PDF using Microsoft Read API. \n",
    "    It's intended for use on PDFs that failed processing by URL alone \n",
    "    due to their size -- e.g., above the max. dimensions that Read API supports. \n",
    "    \n",
    "    Passed a URL that points to the PDF\n",
    "    Downloads the PDF data to a temporary file (temp_pdf_fp), resizing pages if needed. \n",
    "    Loads and passes the data to a Read API call. \n",
    "    Deletes the temp file. \n",
    "    \n",
    "    Note: optional argument to only get the first page, sometimes useful for testing. \n",
    "    \n",
    "    Returns whether call was successful, as well as the JSON output of the OCR call. \n",
    "    '''\n",
    "    \n",
    "    pdf_data = requests.get(final_dest_url).content\n",
    "    \n",
    "    # A hacky way of figuring out whether the link points to a PDF or to an \"error\" page\n",
    "    if not \"%PDF\" in str(pdf_data): \n",
    "        return \"ERROR: URL does not point to PDF.\", False\n",
    "    \n",
    "    # Use PyPDF2 to read the PDF data and write out a copy of that data, \n",
    "    # with pages resized if needed\n",
    "    writer = PyPDF2.PdfFileWriter()\n",
    "    reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "    \n",
    "    for i in range(reader.numPages): # loop through all pages\n",
    "        page = reader.getPage(i)\n",
    "        width = float(page.mediaBox.lowerRight[0]) \n",
    "        height = float(page.mediaBox.upperLeft[1]) \n",
    "        # resize if width or height of page is too large\n",
    "        if width > MAX_DIM: \n",
    "            width = MAX_DIM\n",
    "            page.scaleTo(MAX_DIM, height)\n",
    "        if height > MAX_DIM: \n",
    "            page.scaleTo(width, MAX_DIM)\n",
    "        writer.addPage(page)\n",
    "        if firstPageOnly:\n",
    "            break\n",
    "    \n",
    "    # create temp file \n",
    "    with open(temp_pdf_fp, \"wb\") as f: \n",
    "        writer.write(f)\n",
    "    \n",
    "    output, success = call_read_api(flag = \"pdf\", image_path = temp_pdf_fp)\n",
    "    \n",
    "    # clean up\n",
    "    os.remove(temp_pdf_fp)\n",
    "    \n",
    "    return output, success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_content(json_output, dest_fn):\n",
    "    '''\n",
    "    Given: JSON output & destination filepath (and filename)\n",
    "    Saves content to the destination.\n",
    "    '''\n",
    "    print('Saving files to {}'.format(dest_fn))\n",
    "\n",
    "    with open(dest_fn, 'w') as outfile:\n",
    "        json.dump(json_output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_ocr(fin_url_sublist, duplicates, failures, flag, \n",
    "             filepath_out = \"/home/dssg-cfa/ke-gazettes/\", temp_pdf_fp = \"temp.pdf\"):\n",
    "    '''\n",
    "    Loops through all final destination URLs in sublist. \n",
    "    Calls Read API to get and save json files for all of them. \n",
    "    Flag should be \"url\" or \"pdf\"\n",
    "    '''\n",
    "    calls = 0\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for final_dest_url in fin_url_sublist:\n",
    "        print(\"starting on call \" + str(calls))\n",
    "        calls += 1\n",
    "        # optional: add param: flag = connected_africa or flag = gazeti\n",
    "        dest_fn = filepath_out + df.get_name(final_dest_url).strip().lower()\n",
    "        if os.path.exists(dest_fn):\n",
    "            print(\"Gazette already exists\")\n",
    "            # append to duplicates; don't call Read API\n",
    "            duplicates.append({dest_fn: final_dest_url})\n",
    "            continue\n",
    "\n",
    "        # ----- CALL READ API AND SAVE OUTPUT ----- \n",
    "        # returns \"analysis\" variable from read API & whether call was successful \n",
    "        \n",
    "        if flag == \"url\":\n",
    "            json_output, success = call_read_api(final_dest_url)\n",
    "        elif flag == \"pdf\":\n",
    "            json_output, success = call_read_api_resize(final_dest_url, temp_pdf_fp)\n",
    "        else: \n",
    "            print(\"Flag must be \\'pdf\\' or \\'url\\'\")\n",
    "\n",
    "        if success: \n",
    "            save_content(json_output, dest_fn)\n",
    "            print(\"success \" + str(count))\n",
    "            count += 1\n",
    "\n",
    "        # ----- ERROR HANDLING: IF OCR CALL DOESN'T GO THROUGH ----- \n",
    "        # append: error message (json_output), date of gazette, permanent image URL \n",
    "        else: \n",
    "            failures.append(final_dest_url)\n",
    "            print('failed' + str(json_output) + \": \" + str(failures[-1]))\n",
    "\n",
    "    time_diff = time.time() - start_time\n",
    "    print(str(time_diff/60) + \" minutes for \" + str(count) + \" gazettes\")\n",
    "    print(\"Failed on \" + str(len(failures)) + \" gazettes.\")\n",
    "    # print(\"Duplicates: \" + str(len(duplicates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
